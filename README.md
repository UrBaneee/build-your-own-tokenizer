# ðŸ§  Build Your Own Tokenizer â€” NLP Tokenization Notes & Code

This repository contains my learning notes and re-implementation of core tokenization concepts in NLP, inspired by Data Science Dojo's tutorial.

I recreated the notebook from scratch, added screenshots (because the original images do not download in `.ipynb`), and wrote my own explanations to deeply understand how text becomes tokens.

---

## ðŸ“š Whatâ€™s Inside

| Topic | Description |
|---|---|
Tokenization basics | Words, characters, subword units  
Vocabulary building | `set()`, indexing, unknown tokens  
Subword tokenization | Why we need subwords  
Byte Pair Encoding (BPE) | Merges, vocab learning, applying BPE  
From text â†’ tokens â†’ embeddings | End-to-end pipeline preview  

---

## ðŸ““ Notebook
- `tokenization_notes.ipynb` â€” My version with explanations & screenshots âœ…

---

## ðŸ§ª Skills practiced
- Python
- Text preprocessing
- Vocabulary construction
- BPE algorithm intuition
- Understanding how LLM tokenizers work

---

## ðŸ“Ž References

Original tutorial by Data Science Dojo  
YouTube: https://www.youtube.com/watch?v=L5CR-k2ROu4&t=1403s  
Original repo: https://github.com/debnsuma/nlp-embeddings/tree/main
> Credit to the original authors. This repo is for study and reinforcement only.

---

## ðŸš€ Why I built this
I want to truly understand the foundations of LLMs â€” not just call APIs.  
This repo documents my learning journey and hands-on practice.

---

## âœ… Next Steps
- Implement BPE from scratch in Python
- Build a small custom tokenizer on my own corpus
- Train a tiny language model on tokenized data

---

## âœ¨ Connect
If you're also studying NLP & LLM systems, feel free to reach out!
